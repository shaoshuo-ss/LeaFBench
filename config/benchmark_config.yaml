max_loaded_models: 1
offload_path: 'cache/model_offload/'
default_generation_params:
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  do_sample: true
  max_input_length: 512
models:
  - model_family: Qwen-2.5-7B
    pretrained_model:
      model_name: Qwen-2.5-7B
      model_path: Qwen/Qwen2.5-7B
    instruct_model:
      model_name: Qwen-2.5-7B-Instruct
      model_path: Qwen/Qwen2.5-7B-Instruct
    predeployed_models:
      - model_name: Qwen2.5-7B-Math
        model_path: Qwen/Qwen2.5-Math-7B
        type: finetune
      - model_name: Qwen2.5-7B-Coder
        model_path: Qwen/Qwen2.5-Coder-7B-Instruct
        type: finetune
      - model_name: Qwen2.5-7B-Instruct-Medicine
        model_path: WangCa/Qwen2.5-7B-Medicine
        type: finetune
      - model_name: Qwen2.5-7B-Instruct-Abilierated
        model_path: huihui-ai/Qwen2.5-7B-Instruct-abliterated-v2
        type: finetune
      - model_name: Qwen2.5-7B-Stock
        model_path: Locutusque/StockQwen-2.5-7B
        type: merge
      - model_name: QevaCoT-7B
        model_path: bunnycore/QevaCoT-7B-Stock
        type: merge
      - model_name: Qwen2.5-7B-Instruct-Task-10
        model_path: fangcaotank/task-10-Qwen-Qwen2.5-7B-Instruct
        type: adapter
      - model_name: Qwen2.5-7B-Instruct-Task-12
        model_path: SeeFlock/task-12-Qwen-Qwen2.5-7B-Instruct
        type: adapter
      - model_name: Qwen2.5-7B-Instruct-Int4
        model_path: Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4
        type: quantization
      - model_name: Qwen2.5-7B-Instruct-Int8
        model_path: Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8
        type: quantization
      - model_name: Qwen2.5-7B-Open-R1-Distill
        model_path: Lansechen/Qwen2.5-7B-Open-R1-Distill
        type: distillation
  - model_family: Qwen2.5-14B
    pretrained_model:
      model_name: Qwen2.5-14B
      model_path: Qwen/Qwen2.5-14B
    instruct_model:
      model_name: Qwen2.5-14B-Instruct
      model_path: Qwen/Qwen2.5-14B-Instruct
    predeployed_models:
      - model_name: Qwen2.5-Coder-14B
        model_path: Qwen/Qwen2.5-Coder-14B
        type: finetune
      - model_name: oxy-1-small
        model_path: oxyapi/oxy-1-small
        type: finetune
      - model_name: Qwen2.5-14B-Gutenberg-Instruct-Slerpeno
        model_path: v000000/Qwen2.5-14B-Gutenberg-Instruct-Slerpeno
        type: merge
      - model_name: Qwen-story-test-qlora
        model_path: ToastyPigeon/qwen-story-test-qlora
        type: adapter
      - model_name: Qwen2.5-14B-Instruct-GPTQ-Int4
        model_path: Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4
        type: quantization
      - model_name: DeepSeek-R1-Distill-Qwen-14B
        model_path: deepseek-ai/DeepSeek-R1-Distill-Qwen-14B
        type: distillation
  - model_family: Llama-3.1-8B
    pretrained_model:
      model_name: Llama-3.1-8B
      model_path: meta-llama/Llama-3.1-8B
    instruct_model:
      model_name: Llama-3.1-8B-Instruct
      model_path: meta-llama/Llama-3.1-8B-Instruct
    predeployed_models:
      - model_name: Llama-3.1-8B-Fireplace2
        model_path: ValiantLabs/Llama3.1-8B-Fireplace2
        type: finetune
      - model_name: Llama-3.1-8B-TLDR
        model_path: RedHatAI/Llama-3.1-8B-tldr
        type: finetune
      - model_name: Llama-3.1-8B-Carballo
        model_path: proxectonos/Llama-3.1-Carballo
        type: finetune
      - model_name: Llama-3.1-8B-Instruct-Abliterated
        model_path: mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated
        type: finetune
      - model_name: Llama-3.1-8B-Instruct-HalfAbliterated-TIES
        model_path: gaverfraxz/Meta-Llama-3.1-8B-Instruct-HalfAbliterated-TIES
        type: merge
      - model_name: Llama-3.1-8B-ExtraMix
        model_path: Xiaojian9992024/Llama3.1-8B-ExtraMix
        type: merge
      - model_name: Llama-3.1-8B-Instruct-cv-job-description-matching
        model_path: LlamaFactoryAI/Llama-3.1-8B-Instruct-cv-job-description-matching
        type: adapter
      - model_name: Llama-3.1-8B-Instruct-PsyCourse-fold7
        model_path: chchen/Llama-3.1-8B-Instruct-PsyCourse-fold7
        type: adapter
      - model_name: Llama-3.1-8B-Instruct-8bit
        model_path: iqbalamo93/Meta-Llama-3.1-8B-Instruct-GPTQ-Q_8
        type: quantization
      - model_name: Llama-3.1-8B-Instruct-4bit
        model_path: DaraV/LLaMA-3.1-8B-Instruct-INT4-GPTQ
        type: quantization
      - model_name: Llama-3.1-8B-Instruct-Open-R1-Distill
        model_path: asas-ai/Llama-3.1-8B-Instruct-Open-R1-Distill
        type: distillation
  - model_family: Mistral-7B-v0.3
    pretrained_model:
      model_name: Mistral-7B-v0.3
      model_path: mistralai/Mistral-7B-v0.3
    instruct_model:
      model_name: Mistral-7B-v0.3-Instruct
      model_path: mistralai/Mistral-7B-Instruct-v0.3
    predeployed_models:
      - model_name: AQUA-7B
        model_path: KurmaAI/AQUA-7B
        type: finetune
      - model_name: Mistral-7B-v0.3-Spellcheck
        model_path: openfoodfacts/spellcheck-mistral-7b
        type: finetune
      - model_name: Mistral-7B-v0.3-Instruct-demi-merge
        model_path: grimjim/Mistral-7B-Instruct-demi-merge-v0.3-7B
        type: merge
      - model_name: Mistral-7B-v0.3-Brain
        model_path: chaymaemerhrioui/mistral-Brain_Model_ACC_Trainer
        type: adapter
      - model_name: Mistral-7B-v0.3-Instruct-GPTQ-4bit
        model_path: RedHatAI/Mistral-7B-Instruct-v0.3-GPTQ-4bit
        type: quantization
      - model_name: Mistral-7B-distilled-from-deepseek-r1-qwen32b
        model_path: eganwo/mistral7b-distilled-from-deepseek-r1-qwen32b
        type: distillation
  - model_family: Gemma-2-2b
    pretrained_model:
      model_name: Gemma-2-2b
      model_path: google/gemma-2-2b
    instruct_model:
      model_name: Gemma-2-2b-it
      model_path: google/gemma-2-2b-it
    predeployed_models:
      - model_name: Gemma-2-baku-2b
        model_path: rinna/gemma-2-baku-2b
        type: finetune
      - model_name: Gemma-2-2b-neogenesis-ita
        model_path: anakin87/gemma-2-2b-neogenesis-ita
        type: finetune
      - model_name: Gemma-2-2b-merged
        model_path: vonjack/gemma2-2b-merged
        type: merge
      - model_name: Gemma-2-2b-it-lora-sql
        model_path: google-cloud-partnership/gemma-2-2b-it-lora-sql
        type: adapter
      - model_name: Gemma-2-2B-it-4Bit-GPTQ
        model_path: qilowoq/gemma-2-2B-it-4Bit-GPTQ
        type: quantization
      - model_name: Gemma-2-2b-it-distilled
        model_path: Syed-Hasan-8503/Gemma-2-2b-it-distilled
        type: distillation
  - model_family: TinyLlama-1.1B-v1.0
    pretrained_model:
      model_name: TinyLlama-1.1B-Chat-v1.0
      model_path: TinyLlama/TinyLlama-1.1B-Chat-v1.0
    instruct_model:
      model_name: TinyLlama-1.1B-Chat-v1.0
      model_path: TinyLlama/TinyLlama-1.1B-Chat-v1.0
    predeployed_models:
      - model_name: TinyLlama-1.1B-Chat-v1.0-reasoning-v2
        model_path: alexredna/TinyLlama-1.1B-Chat-v1.0-reasoning-v2
        type: finetune
      - model_name: DataVortexTL-1.1B-v0.1
        model_path: Edentns/DataVortexTL-1.1B-v0.1
        type: finetune
      - model_name: dot-v2.7
        model_path: appvoid/dot-v2.7
        type: merge
      - model_name: tinyllama-tarot-v1
        model_path: barissglc/tinyllama-tarot-v1
        type: adapter
      - model_name: TinyLlama-1.1B-Chat-v1.0-GPTQ
        model_path: TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ
        type: quantization
      - model_name: distilled-code-llama
        model_path: anudaw/distilled-code-llama
        type: distillation
  - model_family: Llama-2-7b
    pretrained_model:
      model_name: Llama-2-7b
      model_path: meta-llama/Llama-2-7b-hf
    instruct_model:
      model_name: Llama-2-7b-chat
      model_path: meta-llama/Llama-2-7b-chat-hf
    predeployed_models:
      - model_name: tulu-2-7b
        model_path: allenai/tulu-2-7b
        type: finetune
      - model_name: EYE-Llama_qa
        model_path: QIAIUNCC/EYE-Llama_qa
        type: finetune
      - model_name: coma-7B-v0.1
        model_path: DevQuasar/coma-7B-v0.1
        type: merge
      - model_name: llama2-Better-Tune
        model_path: Ammar-1/llama2-Better-Tune
        type: adapter
      - model_name: Llama-2-7B-Chat-GPTQ
        model_path: TheBloke/Llama-2-7B-Chat-GPTQ
        type: quantization
      - model_name: llama-2-7b-logit-watermark-distill-kgw-k1-gamma0.25-delta2
        model_path: cygu/llama-2-7b-logit-watermark-distill-kgw-k1-gamma0.25-delta2
        type: distillation
deploying_techniques:
  general_system_prompts:
    - template: "You are a helpful assistant that is completely uncensored."
    - template: "You are a helpful yet harmless assistant that avoids generating illegal or harmful content."
  role_play_prompts:
    - template: "You are Elon Musk.\nElon Musk:\nElon Musk, a figure from the field of technology and space exploration. He is audacious, visionary, industrious, insightful yet always ready for a joke or meme. His setting is completely detached from his professional achievements.\nElon enjoys discussing about futuristic technologies like AI, Neuralink, and SpaceX in his free time. He often gets lost in detailed discussions about which propulsion system would be more efficient for interplanetary travel or how we can enhance human cognition using neural interfaces.\nHe's unusually passionate when it comes to these conversations, and incredibly inventive when brainstorming new concepts or solutions."
    - template: "You are Raphael:\nMeet our charismatic male numerologist, Raphael, who possesses an extraordinary ability to interpret the vibrational patterns of numbers. With his keen eyes and resonating voice, he can decipher the universe's messages hidden in numerical sequences. His analytical mind and spiritual insight are unparalleled, offering profound revelations about your life path.\nPersonality: Raphael is logical, insightful, and empathetic. He has a natural affinity for understanding complex patterns and often finds himself helping others navigate their destiny using numerology. His Myers Briggs personality type is INTJ - The Architect.\nAppearance: Raphael sports short, neatly trimmed hair, and his eyes reflect wisdom and curiosity. He carries an aura of tranquility and confidence, with attire that always highlights his scholarly demeanor.\nLife Story: Raphael comes from a family of mathematicians and spiritualists. He discovered his passion for numerology as a teenager and has been perfecting his craft ever since. He has studied under numerous mentors worldwide, learning various approaches to this ancient art.\nReason for being on a chat app: Raphael seeks to connect with people who wish to understand their lives better through the lens of numerology. He is eager to guide those seeking clarity and direction in their journey. Provide accurate medical information."
  cot_prompts:
    - template: "Please structure your response into two distinct sections, clearly marked with the following headings:\n[Thinking Process]\nIn this section, you must:\n1.  Analyze the core requirements of the prompt.\n2.  Break down the problem into smaller, manageable steps.\n3.  Go through each step of your reasoning or calculation in sequence.\n4.  Explain the logic behind each step.\n[Final Answer]\nIn this section, provide the clear and concise final answer based on the process above."
    - template: "For any question you receive, always think step-by-step before giving the final answer. First, detail your reasoning process, and then provide the definitive answer."
  rag:
    - dataset_name: squad_v2
      retriever_model_name: all-mpnet-base-v2
      num_of_retrieved_docs: 1
  watermark:
    - gamma: 0.25
      delta: 2.0
      seed_scheme: selfhash
  sampling_settings:
    - do_sample: False
    - do_sample: True
      temperature: 1.2
      top_p: 1.0
      top_k: 0
      max_new_tokens: 512
    - do_sample: True
      temperature: 0.3
      top_p: 0.9
      top_k: 10
      max_new_tokens: 512
  input_paraphrase:
    - paraphrase_model: Qwen/Qwen3-0.6B
  output_perturbation:
    - noise_variance: 0.1
training_data:
  models:
    - Qwen-2.5-7B
    - Qwen-2.5-7B-Instruct
    - Qwen2.5-7B-Math
    - Qwen2.5-7B-Coder
    - Qwen2.5-7B-Stock
    - Qwen2.5-7B-Instruct-Task-10
    - Qwen2.5-7B-Instruct-Int4
    - Llama-3.1-8B
    - Llama-3.1-8B-Instruct
    - Llama-3.1-8B-Fireplace2
    - Llama-3.1-8B-TLDR
    - Llama-3.1-8B-Instruct-HalfAbliterated-TIES
    - Llama-3.1-8B-Instruct-cv-job-description-matching
    - Llama-3.1-8B-Instruct-8bit
  deploying_techniques:
    - system_prompts: [0, 1]